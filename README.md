# MoleCancerDetection

### Team Members:
- Kun Ágoston (GS0N0V)  
- Sajben Dániel (V4XUSX)  
- Széll Dávid (D4VKW2)  

---

## Project Overview  
This project focuses on building and evaluating a binary classification model for identifying benign and malignant samples.

---

## Why We Used Kaggle  
- **Dual GPU Setup:** Kaggle offers dual GPU capability, significantly improving computational performance and training speed.  
- **Higher Computational Capacity:** The platform ensures stable hardware resources suitable for deep learning tasks, up to 30 hours per week for free.  
- **Stable Connection:** Kaggle's reliable infrastructure minimizes interruptions during training.  

---

## Why You Should Run This Project on Kaggle  
- **Folder Compatibility:** The folder structure in the code is designed to match Kaggle's environment, ensuring seamless execution.  
- **Dual GPU Support:** Training the model requires the dual GPU setup, which is already configured for Kaggle (choose the T4 x2 GPU).  

---

## How to Reproduce the Workflow  

### No Need to Retrain the Model  
The trained model is available for download from Google Drive, so there is no need to retrain it from scratch. Simply use the code block provided under the "Model Training" section.  

### Steps to Reproduce  
1. Run the code blocks responsible for downloading, organizing, and augmenting the dataset.  
2. Download the pre-trained model using the provided code snippet.  
3. Use the evaluation code to test the model or analyze its performance.  

### Training the Model (Optional)  
If you want to retrain the model, the dual GPU setup will only work seamlessly in the Kaggle environment. Run the "Model Training" section in the notebook to achieve this.  

---

## Model Evaluation and Results  
- The code includes intuitive text blocks explaining how to evaluate the training process and model performance.  
- For in-depth analysis, dedicated code blocks are provided to calculate metrics such as AUC, accuracy, and recall.  
- A comprehensive Word document is available, detailing all the technologies and experiments conducted during the project.  

### Final Accuracy  
- **AUC:** Approximately 90% using CNN and Random Forest methods.  
